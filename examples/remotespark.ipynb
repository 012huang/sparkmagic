{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RemoteSparkMagics module is not an IPython extension.\n"
     ]
    }
   ],
   "source": [
    "%load_ext RemoteSparkMagics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info for running sparkmagic:\n",
      "    mode=normal\n",
      "    Possible endpoints are: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%sparkconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info for running sparkmagic:\n",
      "    mode=normal\n",
      "    Possible endpoints are: ['test']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%sparkconf add test url=https://testdnsname.azurehdinsight.net/livy;username=admin;password=Pa$$word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info for running sparkmagic:\n",
      "    mode=normal\n",
      "    Possible endpoints are: ['test', 'prod']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%sparkconf add prod url=https://proddnsname.azurehdinsight.net/livy;username=admin;password=Pa$$word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.14012\n"
     ]
    }
   ],
   "source": [
    "%%sparkmagic -c test\n",
    "val NUM_SAMPLES = 100000;\n",
    "val count = sc.parallelize(1 to NUM_SAMPLES).map { i =>\n",
    "val x = Math.random();\n",
    "val y = Math.random();\n",
    "if (x*x + y*y < 1) 1 else 0\n",
    "}.reduce(_ + _);\n",
    "println(\"Pi is roughly \" + 4.0 * count / NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.156800\n"
     ]
    }
   ],
   "source": [
    "%%sparkmagic -l pyspark -c prod\n",
    "\n",
    "import random\n",
    "from __future__ import print_function\n",
    "\n",
    "NUM_SAMPLES = 100000\n",
    "\n",
    "def sample(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: org.apache.spark.sql.DataFrame = [tableName: string, isTemporary: boolean]\n"
     ]
    }
   ],
   "source": [
    "%%sparkmagic -l sql -m debug -c test\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info for running sparkmagic:\n",
      "    mode=normal\n",
      "    Possible endpoints are: ['test']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%sparkconf delete prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info for running sparkmagic:\n",
      "    mode=debug\n",
      "    Possible endpoints are: ['test']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%sparkconf mode debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res2: Array[(String, Int)] = Array((22:13:19,59), (0:13:19,59), (11:45:56,42), (SystemAge,1), (19,1060), (3:43:51,59), (6/6/13,267), (62,291), (1:33:07,59), (6/22/13,266), (8:13:20,59), (3:00:01,59), (6/24/13,266), (22:45:56,28), (15:13:19,59), (10:00:01,59), (28,275), (59,296), (22:13:20,59), (10:33:07,58), (6/26/13,266), (13,1064), (24,256), (21:00:01,59), (8:13:19,59), (12:00:01,59), (11,1096), (4:13:20,59), (15:13:20,59), (0:45:56,42), (7:00:01,59), (6/28/13,266), (6:13:20,59), (2:45:56,42), (60,295), (23:33:07,58), (6:45:56,42), (20,1050), (20:13:20,59), (14:00:01,59), (7:33:07,58), (19:45:56,28), (5:43:51,59), (71,334), (79,304), (9:33:07,58), (12:33:07,58), (13:13:19,59), (22,255), (26,265), (19:13:19,59), (73,300), (Date,1), (6/13/13,267), (5:00:01,59), (11:13:20,59), (1:00:01,5...\n"
     ]
    }
   ],
   "source": [
    "%%sparkmagic\n",
    "val textFile = sc.textFile(\"wasb://agusparkeight@agushvohrastore.blob.core.windows.net/HdiSamples/SensorSampleData/hvac/HVAC.csv\")\n",
    "val counts = textFile.flatMap(line => line.split(\",\")).map(word => (word, 1)).reduceByKey(_ + _)\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info for running sparkmagic:\n",
      "    mode=debug\n",
      "    Possible endpoints are: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%sparkconf cleanup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}